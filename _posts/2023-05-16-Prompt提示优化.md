---
layout: post
title:  Prompt提示优化
description: "提示工程不仅仅是关于设计和研发提示词。它包含了与大语言模型交互和研发的各种技能和技术。提示工程在实现和大语言模型交互、对接，以及理解大语言模型能力方面都起着重要作用。"
modified: 2023-05-16T11:00:15-06:00
tags: [Prompt优化, 提示工程, Prompt提示,LLM] 

---

提示工程（Prompt Engineering）是一门较新的学科，关注提示词开发和优化，帮助用户将大语言模型（Large Language Model, LLM）用于各场景和研究领域。 





<!-- more -->

   * [技巧1：To Do and Not To](#技巧1to-do-and-not-to)
   * [技巧2：增加示例](#技巧2增加示例)
   * [技巧3：使用引导词，引导模型输出特定内容](#技巧3使用引导词引导模型输出特定内容)
   * [技巧4：增加 Role（角色）或人物](#技巧4增加-role角色或人物)
   * [技巧5：使用特殊符号指令和需要处理的文本分开](#技巧5使用特殊符号指令和需要处理的文本分开)
   * [技巧6：Zero-Shot Chain of Thought](#技巧6zero-shot-chain-of-thought)
   * [技巧7：Few-Shot Chain of Thought](#技巧7few-shot-chain-of-thought)

 研究人员可利用提示工程来提升大语言模型处理复杂任务场景的能力，如问答和算术推理能力。开发人员可通过提示工程设计、研发强大的工程技术，实现和大语言模型或其他生态工具的高效接轨。

本文记录学习Prompt过程中一些重要提示词。

## 技巧1：To Do and Not To

OpenAI 的 API 最佳实践文档里，提到了一个这样的最佳实践：

Instead of just saying what not to do, say what to do instead. 与其告知模型不能干什么，不妨告诉模型能干什么。

我自己的实践是，虽然现在最新的模型已经理解什么是 Not Todo ，但如果你想要的是明确的答案，加入更多限定词，告知模型能干什么，回答的效率会更高，且预期会更明确。

## 技巧2：增加示例

直接告知 AI 什么能做，什么不能做外。在某些场景下，我们能比较简单地向 AI 描述出什么能做，什么不能做。但有些场景，有些需求很难通过文字指令传递给 AI，即使描述出来了，AI 也不能很好地理解。

## 技巧3：使用引导词，引导模型输出特定内容

在代码生成场景里，有一个小技巧，上面提到的案例，其 prompt 还可以继续优化，在 prompt 最后，增加一个代码的引导，告知 AI 我已经将条件描述完了，你可以写代码了。

## 技巧4：增加 Role（角色）或人物

前面提到的改写例子，我在 prompt 里加了 Role 让其更易于阅读，这个优化方法是 OK 的。也很常用，比如你想将你写的 Email 改得更商务，则只需要加 business 相关的词即可。

我再介绍一个更有效的技巧，就是在 prompt 里增加一些 role（角色）相关的内容，让 AI 生成的内容更符合你的需求。

## 技巧5：使用特殊符号指令和需要处理的文本分开

不管是信息总结，还是信息提取，你一定会输入大段文字，甚至多段文字，此时有个小技巧。

可以用###将指令和文本分开。根据我的测试，如果你的文本有多段，增加###会提升 AI 反馈的准确性（这个技巧来自于 OpenAI 的 API 最佳实践文档）

像我们之前写的 prompt 就属于 Less effective prompt。为什么呢？据我的测试，主要还是 AI 不知道什么是指令，什么是待处理的内容，用符号分隔开来会更利于 AI 区分。

## 技巧6：Zero-Shot Chain of Thought

基于上述的第三点缺点，研究人员就找到了一个叫 Chain of Thought 的技巧。

这个技巧使用起来非常简单，只需要在问题的结尾里放一句 Let‘s think step by step （让我们一步步地思考），模型输出的答案会更加准确。

这个技巧来自于 Kojima 等人 2022 年的论文 Large Language Models are Zero-Shot Reasoners。在论文里提到，当我们向模型提一个逻辑推理问题时，模型返回了一个错误的答案，但如果我们在问题最后加入 Let‘s think step by step 这句话之后，模型就生成了正确的答案。

论文里有讲到原因，感兴趣的朋友可以去看看，我简单解释下为什么（🆘 如果你有更好的解释，不妨反馈给我）：

首先各位要清楚像 ChatGPT 这类产品，它是一个统计语言模型，本质上是基于过去看到过的所有数据，用统计学意义上的预测结果进行下一步的输出（这也就是为什么你在使用 ChatGPT 的时候，它的答案是一个字一个字地吐出来，而不是直接给你的原因，因为答案是一个字一个字算出来的）。
当它拿到的数据里有逻辑，它就会通过统计学的方法将这些逻辑找出来，并将这些逻辑呈现给你，让你感觉到它的回答很有逻辑。
在计算的过程中，模型会进行很多假设运算（不过暂时不知道它是怎么算的）。比如解决某个问题是从 A 到 B 再到 C，中间有很多假设。
它第一次算出来的答案错误的原因，只是因为它在中间跳过了一些步骤（B）。而让模型一步步地思考，则有助于其按照完整的逻辑链（A > B > C）去运算，而不会跳过某些假设，最后算出正确的答案。

## 技巧7：Few-Shot Chain of Thought

要解决这个缺陷，就要使用到新的技巧，Few-Shot Chain of Thought。

根据 Wei 他们团队在 2022 年的研究表明：通过向大语言模型展示一些少量的样例，并在样例中解释推理过程，大语言模型在回答提示时也会显示推理过程。这种推理的解释往往会引导出更准确的结果。
